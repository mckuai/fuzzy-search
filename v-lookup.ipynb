{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148ea521",
   "metadata": {},
   "source": [
    "# V-Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "418f903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the Python libraries that we are going to use.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sys import exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2eb7b",
   "metadata": {},
   "source": [
    "## Using the Email parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0e95d",
   "metadata": {},
   "source": [
    "### Cleansing of SF contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d417703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the file that contains the SF contacts.\n",
    "\n",
    "df1 = pd.read_csv('input/sf_contacts.csv', encoding='utf-8').dropna(subset=['Contact ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f65382b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We change the format of the Last Modified Date column from %d/%m/%Y to %Y-%m-%d.\n",
    "\n",
    "df1['Last Modified Date'] = pd.to_datetime(df1['Last Modified Date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e57b826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a DataFrame without NaN values in the Email column.\n",
    "# We create a list with the values that not contain '@'.\n",
    "# We remove those values without '@'.\n",
    "# We sort by Email and then by Last Modified Date.\n",
    "# We drop duplicates by Email and we keep the first one.\n",
    "\n",
    "df1_email = df1.dropna(subset=['Email'])\n",
    "bad_email = df1_email[~df1_email['Email'].str.lower().str.contains('@')]['Email'].tolist()\n",
    "df1_email = df1_email[~df1_email['Email'].isin(bad_email)].reset_index(drop=True)\n",
    "df1_email = df1_email.sort_values(by=['Email', 'Last Modified Date'], ascending=[True, False])\n",
    "df1_email = df1_email.drop_duplicates(subset='Email', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074beef",
   "metadata": {},
   "source": [
    "### Cleansing of external contacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3211bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the file that contains the external contacts. \n",
    "\n",
    "df3 = pd.read_csv('input/ext_data.csv', encoding='utf-8' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d4b403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We clean the LinkedIn URL column if the external file has it. \n",
    "\n",
    "if 'LinkedIn URL' in df3.columns:\n",
    "    for i in range(len(df3)):\n",
    "        if 'linkedin.com' in str(df3['LinkedIn URL'][i]):\n",
    "            df3['LinkedIn URL'][i] = 'www.linkedin.com' + df3['LinkedIn URL'][i].split('linkedin.com')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbd05f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new DataFrame without NaN values in the Email column.\n",
    "# We remove duplicate contacts by Email.\n",
    "\n",
    "df3_email = df3.dropna(subset=['Email'])\n",
    "df3_email = df3_email.drop_duplicates(subset='Email', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef50f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge the two DataFrames by Email. \n",
    "\n",
    "inner_email = pd.merge(df1_email,\n",
    "                      df3_email,\n",
    "                      on = 'Email',\n",
    "                      how = 'inner',\n",
    "                      indicator = True).drop(columns = '_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be014248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new list with the inner values. \n",
    "\n",
    "inner_email_list = inner_email['Email'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fe01a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_email_0 = inner_email[['Email', 'Contact ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79dd3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = pd.merge(inner_email_0,\n",
    "                      df3_email,\n",
    "                      on = 'Email',\n",
    "                      how = 'inner',\n",
    "                      indicator = True).drop(columns = '_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5ccd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create two DataFrames with the external information that contains new and old contacts. \n",
    "\n",
    "# old = df3[df3['Email'].isin(inner_email_list)].reset_index(drop=True)\n",
    "new = df3[~df3['Email'].isin(inner_email_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94e4f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We export to the output folder. \n",
    "\n",
    "old.to_csv('output/old_contacts.csv', index = False, encoding = 'utf-8-sig')\n",
    "new.to_csv('output/new_contacts.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f13988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'LinkedIn URL' in df3.columns:\n",
    "    1+1\n",
    "else:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d3686",
   "metadata": {},
   "source": [
    "## Using the LinkedIn URL parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c3615",
   "metadata": {},
   "source": [
    "### Cleansing the old-new DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8390031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a DataFrame without NaN values in the LinkedIn column.\n",
    "# We create a list with values that not contain 'linkedin.com'.\n",
    "# We remove those values without 'linkedin.com'\n",
    "# We clean the LinkedIn URL column.\n",
    "# We sort by LinkedIn URL and then by Last Modified Date.\n",
    "# We drop duplicates by LinkedIn URL and we keep the first one.\n",
    "\n",
    "df1_ln = df1.dropna(subset=['LinkedIn URL'])\n",
    "bad_ln = df1_ln[~df1_ln['LinkedIn URL'].str.lower().str.contains('linkedin.com')]['LinkedIn URL'].tolist()\n",
    "df1_ln = df1_ln[~df1_ln['LinkedIn URL'].isin(bad_ln)].reset_index(drop=True)\n",
    "df1_ln['LinkedIn URL'] = ('www.linkedin.com'+df1_ln['LinkedIn URL'].str.split('.linkedin.com').str[-1]).str.strip()\n",
    "df1_ln = df1_ln.sort_values(by=['LinkedIn URL', 'Last Modified Date'], ascending=[True, False])\n",
    "df1_ln = df1_ln.drop_duplicates(subset='LinkedIn URL', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e286e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a DataFrame (from 'old' DataFrame) without NaN values in the LiknedIn URL column.\n",
    "# We create a list with values that not contain 'linkedin.com'.\n",
    "# We remove those values. \n",
    "# We drop duplicates by LinkedIn URL.\n",
    "\n",
    "old_ln = old.dropna(subset=['LinkedIn URL'])\n",
    "bad_ln_old = old_ln[~old_ln['LinkedIn URL'].str.lower().str.contains('linkedin.com')]['LinkedIn URL'].tolist()\n",
    "old_ln = old_ln[~old_ln['LinkedIn URL'].isin(bad_ln_old)].reset_index(drop=True)\n",
    "old_ln = old_ln.drop_duplicates(subset='LinkedIn URL', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6e16be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a DataFrame (from 'new' DataFrame) without NaN values in the LiknedIn URL column.\n",
    "# We create a list with values that not contain 'linkedin.com'.\n",
    "# We remove those values. \n",
    "# We drop duplicates by LinkedIn URL.\n",
    "\n",
    "new_ln = new.dropna(subset=['LinkedIn URL'])\n",
    "bad_ln_new = new_ln[~new_ln['LinkedIn URL'].str.lower().str.contains('linkedin.com')]['LinkedIn URL'].tolist()\n",
    "new_ln = new_ln[~new_ln['LinkedIn URL'].isin(bad_ln_new)].reset_index(drop=True)\n",
    "new_ln = new_ln.drop_duplicates(subset='LinkedIn URL', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72d9cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge df1_ln and old_ln by 'LinkedIn URL'.\n",
    "\n",
    "inner_ln_old = pd.merge(df1_ln,\n",
    "                      old_ln,\n",
    "                      on = 'LinkedIn URL',\n",
    "                      how = 'inner',\n",
    "                      indicator = True).drop(columns = '_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e958bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge df1_ln and new_ln by 'LinkedIn URL'.\n",
    "\n",
    "inner_ln_new = pd.merge(df1_ln,\n",
    "                      new_ln,\n",
    "                      on = 'LinkedIn URL',\n",
    "                      how = 'inner',\n",
    "                      indicator = True).drop(columns = '_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43d71fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create inner lists.\n",
    "\n",
    "inner_ln_new_list = inner_ln_new['LinkedIn URL'].tolist()\n",
    "inner_ln_old_list = inner_ln_old['LinkedIn URL'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2d00bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create DataFrames; old_new means old Email and new LinkedIn. \n",
    "\n",
    "old_new = old[~old['LinkedIn URL'].isin(inner_ln_old_list)].reset_index(drop=True)\n",
    "new_new = new[~new['LinkedIn URL'].isin(inner_ln_new_list)].reset_index(drop=True)\n",
    "old_old = old[old['LinkedIn URL'].isin(inner_ln_old_list)].reset_index(drop=True)\n",
    "new_old = new[new['LinkedIn URL'].isin(inner_ln_new_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88b00492",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_old2 = df1_ln[df1_ln['LinkedIn URL'].isin(inner_ln_new_list)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2579ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_old2.drop(['Email', 'Last Modified Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc05ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_ln_new2 = pd.merge(new_old,\n",
    "                        new_old2,\n",
    "                        on = 'LinkedIn URL',\n",
    "                        how = 'inner',indicator = True).drop(columns = '_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6df5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [old_new, old_old, inner_ln_new2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbf098d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a4bff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_old2 = result.drop_duplicates(subset='Contact ID', keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9189fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save only the new_new DataFrame because contains new Emails and new LinkedIn URL.\n",
    "\n",
    "new_new.to_csv('output/new_new_contacts.csv', encoding='utf-8-sig', index=False)\n",
    "old_old2.to_csv('output/old_old_contacts.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4765d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "969bc0702dbd192e22f28477f3f72e240f90fe5cfe3a0aaa4e512f9f13b64617"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('chamba')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
